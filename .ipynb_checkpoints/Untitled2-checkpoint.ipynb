{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82dadc77-45af-49c0-89ce-52ed9dce346d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\myazi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     CrossEncoder,\n\u001b[0;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\backend.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Callable, Literal\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[0;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\util.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[0;32m     24\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\myazi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re # Para ayudar en la limpieza de texto si es necesario\n",
    "\n",
    "# --- Configuraci칩n Mejorada ---\n",
    "PDF_PATH = \"Prueba4.pdf\"\n",
    "MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# --- Par치metros de Chunking ---\n",
    "# Tama침o objetivo de cada fragmento de texto (en caracteres)\n",
    "CHUNK_SIZE = 700\n",
    "# Solapamiento entre fragmentos consecutivos (en caracteres)\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- Par치metros de B칰squeda ---\n",
    "# Cu치ntos chunks relevantes devolver como m치ximo\n",
    "TOP_K = 3\n",
    "# Umbral de similitud M칈NIMO para considerar un chunk relevante\n",
    "SIMILARITY_THRESHOLD = 0.5 # Puedes ajustar esto. Quiz치s bajarlo un poco al usar chunks m치s peque침os.\n",
    "\n",
    "# --- Funciones Auxiliares ---\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model(model_name):\n",
    "    \"\"\"Carga el modelo de Sentence Transformer.\"\"\"\n",
    "    print(f\"Intentando cargar el modelo: {model_name}...\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(\"Modelo cargado exitosamente.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error cr칤tico al cargar el modelo '{model_name}'. Verifica la instalaci칩n y el nombre del modelo.\")\n",
    "        st.exception(e)\n",
    "        st.stop()\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Divide un texto largo en fragmentos m치s peque침os con solapamiento.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        # Avanza para el siguiente chunk, retrocediendo por el overlap\n",
    "        # Asegura que no haya un bucle infinito si overlap >= size\n",
    "        next_start = start + chunk_size - chunk_overlap\n",
    "        if next_start <= start: # Previene bucle si overlap es muy grande o size peque침o\n",
    "             next_start = start + 1 # Avanza al menos 1 caracter\n",
    "        start = next_start\n",
    "\n",
    "    # Eliminar chunks muy peque침os que puedan quedar al final si son solo overlap\n",
    "    # return [chk for chk in chunks if len(chk.strip()) > chunk_overlap / 2] # Opcional\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@st.cache_data # Cache basado en el contenido del PDF y par치metros de chunking\n",
    "def load_and_process_pdf_chunked(pdf_path, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Lee el PDF, extrae texto, lo limpia y lo divide en chunks.\"\"\"\n",
    "    print(f\"Procesando PDF y dividiendo en chunks: {pdf_path} (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        st.error(f\"Error: El archivo '{pdf_path}' no se encuentra.\")\n",
    "        return [], []\n",
    "\n",
    "    doc_chunks_data = []\n",
    "    all_texts = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        if doc.page_count == 0:\n",
    "            st.warning(f\"El PDF '{pdf_path}' parece estar vac칤o.\")\n",
    "            doc.close()\n",
    "            return [], []\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            page_text = page.get_text(\"text\", sort=True)\n",
    "            # Limpieza b치sica (opcional, ajustar seg칰n necesidad)\n",
    "            page_text = re.sub(r'\\s+', ' ', page_text).strip() # Reemplaza m칰ltiples espacios/saltos con uno solo\n",
    "\n",
    "            if page_text:\n",
    "                page_chunks = chunk_text(page_text, chunk_size, chunk_overlap)\n",
    "                for i, chunk in enumerate(page_chunks):\n",
    "                    if chunk.strip(): # Asegurarse que el chunk no est칠 vac칤o\n",
    "                        doc_chunks_data.append({\n",
    "                            \"text\": chunk.strip(),\n",
    "                            \"page\": page_num + 1,\n",
    "                            # Podr칤as a침adir inicio/fin del chunk si es 칰til\n",
    "                            # \"chunk_index_on_page\": i\n",
    "                        })\n",
    "                        all_texts.append(chunk.strip())\n",
    "            else:\n",
    "                 print(f\"P치gina {page_num + 1} sin texto extra칤ble.\")\n",
    "\n",
    "        doc.close()\n",
    "        print(f\"PDF procesado. {len(doc_chunks_data)} chunks generados.\")\n",
    "\n",
    "        if not doc_chunks_data:\n",
    "             st.warning(f\"No se pudieron generar chunks de texto desde '{pdf_path}'. 쮼l PDF tiene texto seleccionable?\")\n",
    "             return [], []\n",
    "\n",
    "        return doc_chunks_data, all_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error al procesar/chunkear el PDF '{pdf_path}'.\")\n",
    "        st.exception(e)\n",
    "        return [], []\n",
    "\n",
    "# Usamos _model como argumento para que el cache de Streamlit se invalide si el modelo cambia\n",
    "@st.cache_data\n",
    "def get_pdf_embeddings(_model, pdf_texts):\n",
    "    \"\"\"Genera embeddings para los chunks de texto del PDF.\"\"\"\n",
    "    if not pdf_texts:\n",
    "        st.warning(\"No hay textos (chunks) para generar embeddings.\")\n",
    "        return None\n",
    "    if _model is None:\n",
    "        st.error(\"Modelo no cargado, no se pueden generar embeddings.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Generando embeddings para {len(pdf_texts)} chunks...\")\n",
    "    try:\n",
    "        # Considerar usar show_progress_bar=True para chunks largos\n",
    "        embeddings = _model.encode(pdf_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "        print(\"Embeddings generados.\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        st.error(\"Error al generar embeddings para los chunks.\")\n",
    "        st.exception(e)\n",
    "        return None\n",
    "\n",
    "# Cambiado para devolver m칰ltiples chunks relevantes\n",
    "def find_relevant_chunks(query, _model, pdf_embeddings, pdf_chunks_data, top_k=TOP_K, threshold=SIMILARITY_THRESHOLD):\n",
    "    \"\"\"Encuentra los K chunks m치s relevantes para la consulta que superan el umbral.\"\"\"\n",
    "    if _model is None or pdf_embeddings is None or not pdf_chunks_data:\n",
    "        st.warning(\"Faltan datos (modelo, embeddings o chunks) para la b칰squeda.\")\n",
    "        return [] # Devuelve lista vac칤a\n",
    "\n",
    "    print(f\"Buscando top-{top_k} chunks para: '{query}' (umbral: {threshold})\")\n",
    "    try:\n",
    "        query_embedding = _model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "        # Calcular similitudes\n",
    "        cosine_scores = util.pytorch_cos_sim(query_embedding, pdf_embeddings)[0]\n",
    "\n",
    "        # Obtener los top_k resultados (칤ndices y scores)\n",
    "        # Asegurarse de que top_k no sea mayor que el n칰mero de chunks\n",
    "        actual_k = min(top_k, len(pdf_chunks_data))\n",
    "        top_results = torch.topk(cosine_scores, k=actual_k)\n",
    "\n",
    "        relevant_chunks = []\n",
    "        scores_found = [] # Para depuraci칩n o mensajes\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            score_item = score.item()\n",
    "            idx_item = idx.item()\n",
    "            scores_found.append(score_item) # Guardar todos los K scores\n",
    "\n",
    "            if score_item >= threshold:\n",
    "                chunk_data = pdf_chunks_data[idx_item]\n",
    "                relevant_chunks.append({\n",
    "                    \"text\": chunk_data[\"text\"],\n",
    "                    \"page\": chunk_data[\"page\"],\n",
    "                    \"score\": score_item\n",
    "                })\n",
    "                print(f\"  - Chunk relevante encontrado (P치gina {chunk_data['page']}, Score: {score_item:.4f})\")\n",
    "            else:\n",
    "                # Si el score m치s alto ya est치 por debajo del umbral, los dem치s tambi칠n lo estar치n (ya que topk ordena)\n",
    "                print(f\"  - Score {score_item:.4f} por debajo del umbral {threshold}. Deteniendo b칰squeda temprana.\")\n",
    "                break # Optimizaci칩n: no seguir si ya bajamos del umbral\n",
    "\n",
    "        if not relevant_chunks:\n",
    "            print(f\"No se encontraron chunks por encima del umbral. Scores m치s altos: {scores_found}\")\n",
    "\n",
    "        # Devolver la lista de chunks relevantes (puede estar vac칤a)\n",
    "        return relevant_chunks, scores_found # Devolvemos scores para mensaje de \"no encontrado\"\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(\"Error durante la b칰squeda de chunks relevantes.\")\n",
    "        st.exception(e)\n",
    "        return [], []\n",
    "\n",
    "\n",
    "# --- Interfaz de Streamlit ---\n",
    "st.set_page_config(page_title=\"Chatbot RRHH Avanzado\", layout=\"wide\")\n",
    "\n",
    "st.title(\"游뱄 Asistente Virtual de Recursos Humanos (v2 - Precisi칩n Mejorada)\")\n",
    "st.caption(f\"Consultando informaci칩n en: {os.path.basename(PDF_PATH)} (Fragmentado para mayor precisi칩n)\")\n",
    "\n",
    "# --- Carga y Procesamiento ---\n",
    "try:\n",
    "    model = load_model(MODEL_NAME)\n",
    "    if model:\n",
    "        pdf_chunks_data, pdf_texts = load_and_process_pdf_chunked(PDF_PATH, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        pdf_embeddings = None\n",
    "        if pdf_texts:\n",
    "            pdf_embeddings = get_pdf_embeddings(model, pdf_texts)\n",
    "        else:\n",
    "            st.warning(\"No se generaron embeddings porque no se extrajo texto 칰til del PDF.\")\n",
    "    else:\n",
    "        st.error(\"La carga del modelo fall칩. El chatbot no puede operar.\")\n",
    "        pdf_chunks_data, pdf_texts, pdf_embeddings = [], [], None\n",
    "\n",
    "except Exception as main_load_error:\n",
    "    st.error(\"Ocurri칩 un error cr칤tico durante la inicializaci칩n.\")\n",
    "    st.exception(main_load_error)\n",
    "    model, pdf_chunks_data, pdf_texts, pdf_embeddings = None, [], [], None\n",
    "\n",
    "\n",
    "# --- 츼rea de Chat ---\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"쮺칩mo puedo ayudarte con nuestros procedimientos de RRHH hoy?\"}]\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        # Usar st.write o st.markdown para permitir formato m치s rico si es necesario\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "user_query = st.chat_input(\"Escribe tu consulta detallada aqu칤...\")\n",
    "\n",
    "if user_query:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(user_query)\n",
    "\n",
    "    # Generar y mostrar respuesta del asistente\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response_container = st.empty()\n",
    "        response_container.write(\"Analizando tu consulta y buscando en el documento...\")\n",
    "\n",
    "        if model is None or pdf_embeddings is None or not pdf_chunks_data:\n",
    "             final_response = f\"Lo siento, no puedo procesar tu consulta en este momento. Hay un problema con la carga del modelo o el procesamiento del documento '{os.path.basename(PDF_PATH)}'.\"\n",
    "             response_container.error(final_response)\n",
    "        else:\n",
    "            # Buscar los chunks relevantes\n",
    "            relevant_chunks, all_top_scores = find_relevant_chunks(\n",
    "                user_query, model, pdf_embeddings, pdf_chunks_data, TOP_K, SIMILARITY_THRESHOLD\n",
    "            )\n",
    "\n",
    "            if relevant_chunks:\n",
    "                # Construir una respuesta unificada\n",
    "                response_parts = [\"Encontr칠 las siguientes secciones que parecen m치s relevantes para tu consulta:\\n\"]\n",
    "                for i, chunk in enumerate(relevant_chunks):\n",
    "                    response_parts.append(f\"--- **Fragmento {i+1} (P치gina {chunk['page']}, Similitud: {chunk['score']:.2f})** ---\")\n",
    "                    response_parts.append(f\"> {chunk['text']}\") # Usar blockquote para el texto\n",
    "                    response_parts.append(\"\\n\") # A침adir espacio\n",
    "\n",
    "                response_parts.append(f\"*Fuente: {os.path.basename(PDF_PATH)}*\")\n",
    "                final_response = \"\\n\".join(response_parts)\n",
    "                response_container.info(final_response) # Usar info para destacar\n",
    "\n",
    "            else:\n",
    "                # Mensaje m치s informativo si no se encontr칩 nada 칰til\n",
    "                max_score_info = \"\"\n",
    "                if all_top_scores:\n",
    "                    max_score_info = f\" La similitud m치s alta encontrada fue de {max(all_top_scores):.2f}, que est치 por debajo del umbral requerido ({SIMILARITY_THRESHOLD}).\"\n",
    "\n",
    "                final_response = (f\"Lo siento, no encontr칠 informaci칩n suficientemente espec칤fica sobre '{user_query}' \"\n",
    "                                  f\"en el documento '{os.path.basename(PDF_PATH)}' con el nivel de confianza necesario.\"\n",
    "                                  f\"{max_score_info} Intenta reformular tu pregunta siendo m치s espec칤fico o usando otras palabras clave.\")\n",
    "                response_container.warning(final_response)\n",
    "\n",
    "        # A침adir la respuesta final al historial\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_response})\n",
    "\n",
    "# --- Nota T칠cnica (opcional) ---\n",
    "# st.sidebar.info(\n",
    "#     f\"**Configuraci칩n:** Modelo: {MODEL_NAME}, Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}, Top K: {TOP_K}, Umbral: {SIMILARITY_THRESHOLD}\"\n",
    "# )\n",
    "\n",
    "# --- LLM Backend Configuration ---\n",
    "# Adapt this URL if your Mistral API endpoint is different (e.g., /v1/chat/completions)\n",
    "MISTRAL_API_URL = \"http://localhost:10001/api/generate\" # Common Ollama endpoint\n",
    "# Replace with the actual model name served by your local instance (e.g., \"mistral\", \"mistral:7b\")\n",
    "MISTRAL_MODEL_NAME = \"gemma3:latest\" # <<< CHANGE THIS IF NEEDED\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d24ae1-7571-4b7e-830d-33b2620917ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
