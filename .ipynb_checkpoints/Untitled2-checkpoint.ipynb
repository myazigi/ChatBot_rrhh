{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82dadc77-45af-49c0-89ce-52ed9dce346d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\myazi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     CrossEncoder,\n\u001b[0;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\backend.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Callable, Literal\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[0;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\util.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[0;32m     24\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_torch_npu_available' from 'transformers' (C:\\Users\\myazi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re # Para ayudar en la limpieza de texto si es necesario\n",
    "\n",
    "# --- Configuración Mejorada ---\n",
    "PDF_PATH = \"Prueba4.pdf\"\n",
    "MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# --- Parámetros de Chunking ---\n",
    "# Tamaño objetivo de cada fragmento de texto (en caracteres)\n",
    "CHUNK_SIZE = 700\n",
    "# Solapamiento entre fragmentos consecutivos (en caracteres)\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- Parámetros de Búsqueda ---\n",
    "# Cuántos chunks relevantes devolver como máximo\n",
    "TOP_K = 3\n",
    "# Umbral de similitud MÍNIMO para considerar un chunk relevante\n",
    "SIMILARITY_THRESHOLD = 0.5 # Puedes ajustar esto. Quizás bajarlo un poco al usar chunks más pequeños.\n",
    "\n",
    "# --- Funciones Auxiliares ---\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model(model_name):\n",
    "    \"\"\"Carga el modelo de Sentence Transformer.\"\"\"\n",
    "    print(f\"Intentando cargar el modelo: {model_name}...\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(\"Modelo cargado exitosamente.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error crítico al cargar el modelo '{model_name}'. Verifica la instalación y el nombre del modelo.\")\n",
    "        st.exception(e)\n",
    "        st.stop()\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Divide un texto largo en fragmentos más pequeños con solapamiento.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        # Avanza para el siguiente chunk, retrocediendo por el overlap\n",
    "        # Asegura que no haya un bucle infinito si overlap >= size\n",
    "        next_start = start + chunk_size - chunk_overlap\n",
    "        if next_start <= start: # Previene bucle si overlap es muy grande o size pequeño\n",
    "             next_start = start + 1 # Avanza al menos 1 caracter\n",
    "        start = next_start\n",
    "\n",
    "    # Eliminar chunks muy pequeños que puedan quedar al final si son solo overlap\n",
    "    # return [chk for chk in chunks if len(chk.strip()) > chunk_overlap / 2] # Opcional\n",
    "    return chunks\n",
    "\n",
    "\n",
    "@st.cache_data # Cache basado en el contenido del PDF y parámetros de chunking\n",
    "def load_and_process_pdf_chunked(pdf_path, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Lee el PDF, extrae texto, lo limpia y lo divide en chunks.\"\"\"\n",
    "    print(f\"Procesando PDF y dividiendo en chunks: {pdf_path} (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        st.error(f\"Error: El archivo '{pdf_path}' no se encuentra.\")\n",
    "        return [], []\n",
    "\n",
    "    doc_chunks_data = []\n",
    "    all_texts = []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        if doc.page_count == 0:\n",
    "            st.warning(f\"El PDF '{pdf_path}' parece estar vacío.\")\n",
    "            doc.close()\n",
    "            return [], []\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            page_text = page.get_text(\"text\", sort=True)\n",
    "            # Limpieza básica (opcional, ajustar según necesidad)\n",
    "            page_text = re.sub(r'\\s+', ' ', page_text).strip() # Reemplaza múltiples espacios/saltos con uno solo\n",
    "\n",
    "            if page_text:\n",
    "                page_chunks = chunk_text(page_text, chunk_size, chunk_overlap)\n",
    "                for i, chunk in enumerate(page_chunks):\n",
    "                    if chunk.strip(): # Asegurarse que el chunk no esté vacío\n",
    "                        doc_chunks_data.append({\n",
    "                            \"text\": chunk.strip(),\n",
    "                            \"page\": page_num + 1,\n",
    "                            # Podrías añadir inicio/fin del chunk si es útil\n",
    "                            # \"chunk_index_on_page\": i\n",
    "                        })\n",
    "                        all_texts.append(chunk.strip())\n",
    "            else:\n",
    "                 print(f\"Página {page_num + 1} sin texto extraíble.\")\n",
    "\n",
    "        doc.close()\n",
    "        print(f\"PDF procesado. {len(doc_chunks_data)} chunks generados.\")\n",
    "\n",
    "        if not doc_chunks_data:\n",
    "             st.warning(f\"No se pudieron generar chunks de texto desde '{pdf_path}'. ¿El PDF tiene texto seleccionable?\")\n",
    "             return [], []\n",
    "\n",
    "        return doc_chunks_data, all_texts\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error al procesar/chunkear el PDF '{pdf_path}'.\")\n",
    "        st.exception(e)\n",
    "        return [], []\n",
    "\n",
    "# Usamos _model como argumento para que el cache de Streamlit se invalide si el modelo cambia\n",
    "@st.cache_data\n",
    "def get_pdf_embeddings(_model, pdf_texts):\n",
    "    \"\"\"Genera embeddings para los chunks de texto del PDF.\"\"\"\n",
    "    if not pdf_texts:\n",
    "        st.warning(\"No hay textos (chunks) para generar embeddings.\")\n",
    "        return None\n",
    "    if _model is None:\n",
    "        st.error(\"Modelo no cargado, no se pueden generar embeddings.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Generando embeddings para {len(pdf_texts)} chunks...\")\n",
    "    try:\n",
    "        # Considerar usar show_progress_bar=True para chunks largos\n",
    "        embeddings = _model.encode(pdf_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "        print(\"Embeddings generados.\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        st.error(\"Error al generar embeddings para los chunks.\")\n",
    "        st.exception(e)\n",
    "        return None\n",
    "\n",
    "# Cambiado para devolver múltiples chunks relevantes\n",
    "def find_relevant_chunks(query, _model, pdf_embeddings, pdf_chunks_data, top_k=TOP_K, threshold=SIMILARITY_THRESHOLD):\n",
    "    \"\"\"Encuentra los K chunks más relevantes para la consulta que superan el umbral.\"\"\"\n",
    "    if _model is None or pdf_embeddings is None or not pdf_chunks_data:\n",
    "        st.warning(\"Faltan datos (modelo, embeddings o chunks) para la búsqueda.\")\n",
    "        return [] # Devuelve lista vacía\n",
    "\n",
    "    print(f\"Buscando top-{top_k} chunks para: '{query}' (umbral: {threshold})\")\n",
    "    try:\n",
    "        query_embedding = _model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "        # Calcular similitudes\n",
    "        cosine_scores = util.pytorch_cos_sim(query_embedding, pdf_embeddings)[0]\n",
    "\n",
    "        # Obtener los top_k resultados (índices y scores)\n",
    "        # Asegurarse de que top_k no sea mayor que el número de chunks\n",
    "        actual_k = min(top_k, len(pdf_chunks_data))\n",
    "        top_results = torch.topk(cosine_scores, k=actual_k)\n",
    "\n",
    "        relevant_chunks = []\n",
    "        scores_found = [] # Para depuración o mensajes\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            score_item = score.item()\n",
    "            idx_item = idx.item()\n",
    "            scores_found.append(score_item) # Guardar todos los K scores\n",
    "\n",
    "            if score_item >= threshold:\n",
    "                chunk_data = pdf_chunks_data[idx_item]\n",
    "                relevant_chunks.append({\n",
    "                    \"text\": chunk_data[\"text\"],\n",
    "                    \"page\": chunk_data[\"page\"],\n",
    "                    \"score\": score_item\n",
    "                })\n",
    "                print(f\"  - Chunk relevante encontrado (Página {chunk_data['page']}, Score: {score_item:.4f})\")\n",
    "            else:\n",
    "                # Si el score más alto ya está por debajo del umbral, los demás también lo estarán (ya que topk ordena)\n",
    "                print(f\"  - Score {score_item:.4f} por debajo del umbral {threshold}. Deteniendo búsqueda temprana.\")\n",
    "                break # Optimización: no seguir si ya bajamos del umbral\n",
    "\n",
    "        if not relevant_chunks:\n",
    "            print(f\"No se encontraron chunks por encima del umbral. Scores más altos: {scores_found}\")\n",
    "\n",
    "        # Devolver la lista de chunks relevantes (puede estar vacía)\n",
    "        return relevant_chunks, scores_found # Devolvemos scores para mensaje de \"no encontrado\"\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(\"Error durante la búsqueda de chunks relevantes.\")\n",
    "        st.exception(e)\n",
    "        return [], []\n",
    "\n",
    "\n",
    "# --- Interfaz de Streamlit ---\n",
    "st.set_page_config(page_title=\"Chatbot RRHH Avanzado\", layout=\"wide\")\n",
    "\n",
    "st.title(\"🤖 Asistente Virtual de Recursos Humanos (v2 - Precisión Mejorada)\")\n",
    "st.caption(f\"Consultando información en: {os.path.basename(PDF_PATH)} (Fragmentado para mayor precisión)\")\n",
    "\n",
    "# --- Carga y Procesamiento ---\n",
    "try:\n",
    "    model = load_model(MODEL_NAME)\n",
    "    if model:\n",
    "        pdf_chunks_data, pdf_texts = load_and_process_pdf_chunked(PDF_PATH, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        pdf_embeddings = None\n",
    "        if pdf_texts:\n",
    "            pdf_embeddings = get_pdf_embeddings(model, pdf_texts)\n",
    "        else:\n",
    "            st.warning(\"No se generaron embeddings porque no se extrajo texto útil del PDF.\")\n",
    "    else:\n",
    "        st.error(\"La carga del modelo falló. El chatbot no puede operar.\")\n",
    "        pdf_chunks_data, pdf_texts, pdf_embeddings = [], [], None\n",
    "\n",
    "except Exception as main_load_error:\n",
    "    st.error(\"Ocurrió un error crítico durante la inicialización.\")\n",
    "    st.exception(main_load_error)\n",
    "    model, pdf_chunks_data, pdf_texts, pdf_embeddings = None, [], [], None\n",
    "\n",
    "\n",
    "# --- Área de Chat ---\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"¿Cómo puedo ayudarte con nuestros procedimientos de RRHH hoy?\"}]\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        # Usar st.write o st.markdown para permitir formato más rico si es necesario\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "user_query = st.chat_input(\"Escribe tu consulta detallada aquí...\")\n",
    "\n",
    "if user_query:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(user_query)\n",
    "\n",
    "    # Generar y mostrar respuesta del asistente\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response_container = st.empty()\n",
    "        response_container.write(\"Analizando tu consulta y buscando en el documento...\")\n",
    "\n",
    "        if model is None or pdf_embeddings is None or not pdf_chunks_data:\n",
    "             final_response = f\"Lo siento, no puedo procesar tu consulta en este momento. Hay un problema con la carga del modelo o el procesamiento del documento '{os.path.basename(PDF_PATH)}'.\"\n",
    "             response_container.error(final_response)\n",
    "        else:\n",
    "            # Buscar los chunks relevantes\n",
    "            relevant_chunks, all_top_scores = find_relevant_chunks(\n",
    "                user_query, model, pdf_embeddings, pdf_chunks_data, TOP_K, SIMILARITY_THRESHOLD\n",
    "            )\n",
    "\n",
    "            if relevant_chunks:\n",
    "                # Construir una respuesta unificada\n",
    "                response_parts = [\"Encontré las siguientes secciones que parecen más relevantes para tu consulta:\\n\"]\n",
    "                for i, chunk in enumerate(relevant_chunks):\n",
    "                    response_parts.append(f\"--- **Fragmento {i+1} (Página {chunk['page']}, Similitud: {chunk['score']:.2f})** ---\")\n",
    "                    response_parts.append(f\"> {chunk['text']}\") # Usar blockquote para el texto\n",
    "                    response_parts.append(\"\\n\") # Añadir espacio\n",
    "\n",
    "                response_parts.append(f\"*Fuente: {os.path.basename(PDF_PATH)}*\")\n",
    "                final_response = \"\\n\".join(response_parts)\n",
    "                response_container.info(final_response) # Usar info para destacar\n",
    "\n",
    "            else:\n",
    "                # Mensaje más informativo si no se encontró nada útil\n",
    "                max_score_info = \"\"\n",
    "                if all_top_scores:\n",
    "                    max_score_info = f\" La similitud más alta encontrada fue de {max(all_top_scores):.2f}, que está por debajo del umbral requerido ({SIMILARITY_THRESHOLD}).\"\n",
    "\n",
    "                final_response = (f\"Lo siento, no encontré información suficientemente específica sobre '{user_query}' \"\n",
    "                                  f\"en el documento '{os.path.basename(PDF_PATH)}' con el nivel de confianza necesario.\"\n",
    "                                  f\"{max_score_info} Intenta reformular tu pregunta siendo más específico o usando otras palabras clave.\")\n",
    "                response_container.warning(final_response)\n",
    "\n",
    "        # Añadir la respuesta final al historial\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": final_response})\n",
    "\n",
    "# --- Nota Técnica (opcional) ---\n",
    "# st.sidebar.info(\n",
    "#     f\"**Configuración:** Modelo: {MODEL_NAME}, Chunk Size: {CHUNK_SIZE}, Overlap: {CHUNK_OVERLAP}, Top K: {TOP_K}, Umbral: {SIMILARITY_THRESHOLD}\"\n",
    "# )\n",
    "\n",
    "# --- LLM Backend Configuration ---\n",
    "# Adapt this URL if your Mistral API endpoint is different (e.g., /v1/chat/completions)\n",
    "MISTRAL_API_URL = \"http://localhost:10001/api/generate\" # Common Ollama endpoint\n",
    "# Replace with the actual model name served by your local instance (e.g., \"mistral\", \"mistral:7b\")\n",
    "MISTRAL_MODEL_NAME = \"gemma3:latest\" # <<< CHANGE THIS IF NEEDED\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d24ae1-7571-4b7e-830d-33b2620917ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
